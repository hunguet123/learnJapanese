import UIKit
import AVFoundation
import Speech
import Network

// MARK: - Models
public struct ReadingModel: Codable {
    var questionType: String
    var questionText: String
    var audio: String
    var image: String
    var correctAnswer: String
    var translation: String
    
    // H√†m kh·ªüi t·∫°o t·ª´ JSON
    static func fromJson(_ jsonString: String) -> ReadingModel? {
        guard let jsonData = jsonString.data(using: .utf8) else {
            return nil
        }
        
        do {
            let decoder = JSONDecoder()
            let model = try decoder.decode(ReadingModel.self, from: jsonData)
            return model
        } catch {
            print("L·ªói gi·∫£i m√£ JSON: \(error)")
            return nil
        }
    }
    
    // H√†m chuy·ªÉn ƒë·ªïi sang JSON
    func toJson() -> String? {
        do {
            let encoder = JSONEncoder()
            encoder.outputFormatting = .prettyPrinted
            let jsonData = try encoder.encode(self)
            return String(data: jsonData, encoding: .utf8)
        } catch {
            print("L·ªói m√£ h√≥a JSON: \(error)")
            return nil
        }
    }
    
    // H√†m kh·ªüi t·∫°o m·∫∑c ƒë·ªãnh
    init(
        questionType: String = "reading",
        questionText: String,
        audio: String,
        image: String,
        correctAnswer: String,
        translation: String
    ) {
        self.questionType = questionType
        self.questionText = questionText
        self.audio = audio
        self.image = image
        self.correctAnswer = correctAnswer
        self.translation = translation
    }
}

public class JapaneseReadingView: UIView {
    
    // MARK: - Callbacks
    public var onReadingResult: ((Bool) -> Void)?
    public var onError: ((Error) -> Void)?
    public var onSkipRequest: (() -> Void)?
    public var onRecordingStateChanged: ((Bool) -> Void)?
    public var onPlaybackStarted: (() -> Void)?
    public var onPlaybackFinished: (() -> Void)?
    
    // MARK: - Public Properties
    public var content: ReadingModel? {
        didSet {
            loadContent()
        }
    }
    
    public var isRecording: Bool = false {
        didSet {
            microButton.isSelected = isRecording
            microButton.tintColor = isRecording ? .systemRed : .systemGreen
            onRecordingStateChanged?(isRecording)
        }
    }
    
    // MARK: - UI Customization Properties
    public var textFont: UIFont {
        get { textLabel.font }
        set { textLabel.font = newValue }
    }
    
    public var translationFont: UIFont {
        get { translationLabel.font }
        set { translationLabel.font = newValue }
    }
    
    public var buttonTintColor: UIColor {
        get { speakerButton.tintColor }
        set {
            speakerButton.tintColor = newValue
            if !isRecording {
                microButton.tintColor = newValue
            }
        }
    }
    
    // MARK: - Private Properties
    private var speechRecognizer: SFSpeechRecognizer?
    private var audioEngine: AVAudioEngine?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var audioPlayer: AVAudioPlayer?
    private var similarityThreshold: Double = 0.8
    private var networkMonitor = NWPathMonitor()
    private var isNetworkAvailable = true
    private var offlineRecognitionActive = false
    private var lastRecognizedText: String = ""
    
    // MARK: - UI Elements
    private lazy var contentStack: UIStackView = {
        let stack = UIStackView()
        stack.axis = .vertical
        stack.spacing = 20
        stack.alignment = .center
        stack.translatesAutoresizingMaskIntoConstraints = false
        return stack
    }()
    
    private lazy var imageView: UIImageView = {
        let imageView = UIImageView()
        imageView.contentMode = .scaleAspectFit
        imageView.clipsToBounds = true
        imageView.layer.cornerRadius = 10
        imageView.translatesAutoresizingMaskIntoConstraints = false
        return imageView
    }()
    
    private lazy var textLabel: UILabel = {
        let label = UILabel()
        label.textColor = AppColors.black
        label.numberOfLines = 0
        label.textAlignment = .center
        label.font = .systemFont(ofSize: 24, weight: .medium)
        label.translatesAutoresizingMaskIntoConstraints = false
        return label
    }()
    
    private lazy var translationLabel: UILabel = {
        let label = UILabel()
        label.numberOfLines = 0
        label.textAlignment = .center
        label.font = .systemFont(ofSize: 16)
        label.textColor = .secondaryLabel
        label.translatesAutoresizingMaskIntoConstraints = false
        return label
    }()
    
    private lazy var controlStack: UIStackView = {
        let stack = UIStackView()
        stack.axis = .horizontal
        stack.spacing = 40
        stack.distribution = .fillEqually
        stack.translatesAutoresizingMaskIntoConstraints = false
        return stack
    }()
    
    private lazy var speakerButton: UIButton = {
        let button = UIButton()
        button.setImage(UIImage(systemName: "speaker.wave.2.fill"), for: .normal)
        button.tintColor = .systemBlue
        button.addTarget(self, action: #selector(speakerTapped), for: .touchUpInside)
        return button
    }()
    
    private lazy var microButton: UIButton = {
        let button = UIButton()
        button.setImage(UIImage(systemName: "mic.circle.fill"), for: .normal)
        button.setImage(UIImage(systemName: "stop.circle.fill"), for: .selected)
        button.tintColor = .systemGreen
        button.addTarget(self, action: #selector(microTapped), for: .touchUpInside)
        return button
    }()
    
    private lazy var skipButton: UIButton = {
        let button = UIButton()
        button.setImage(UIImage(systemName: "forward.fill"), for: .normal)
        button.tintColor = .systemGray
        button.addTarget(self, action: #selector(skipTapped), for: .touchUpInside)
        return button
    }()
    
    private lazy var statusLabel: UILabel = {
        let label = UILabel()
        label.numberOfLines = 1
        label.textAlignment = .center
        label.font = .systemFont(ofSize: 12)
        label.textColor = .secondaryLabel
        label.translatesAutoresizingMaskIntoConstraints = false
        label.isHidden = true
        return label
    }()
    
    // MARK: - Initialization
    public override init(frame: CGRect) {
        super.init(frame: frame)
        setupUI()
        setupConstraints()
        setupSpeechRecognition()
        startNetworkMonitoring()
    }
    
    required init?(coder: NSCoder) {
        super.init(coder: coder)
        setupUI()
        setupConstraints()
        setupSpeechRecognition()
        startNetworkMonitoring()
    }
    
    deinit {
        stopRecording()
        audioPlayer?.stop()
        audioPlayer = nil
        networkMonitor.cancel()
    }
    
    // MARK: - UI Setup
    private func setupUI() {
        backgroundColor = .white
        
        addSubview(contentStack)
        contentStack.addArrangedSubview(imageView)
        contentStack.addArrangedSubview(textLabel)
        contentStack.addArrangedSubview(translationLabel)
        contentStack.addArrangedSubview(statusLabel)
        contentStack.addArrangedSubview(controlStack)
        
        controlStack.addArrangedSubview(speakerButton)
        controlStack.addArrangedSubview(microButton)
        controlStack.addArrangedSubview(skipButton)
    }
    
    private func setupConstraints() {
        NSLayoutConstraint.activate([
            contentStack.topAnchor.constraint(equalTo: topAnchor, constant: 20),
            contentStack.leadingAnchor.constraint(equalTo: leadingAnchor, constant: 20),
            contentStack.trailingAnchor.constraint(equalTo: trailingAnchor, constant: -20),
            contentStack.bottomAnchor.constraint(equalTo: bottomAnchor, constant: -20),
            
            imageView.heightAnchor.constraint(equalToConstant: 50),
            imageView.widthAnchor.constraint(equalTo: contentStack.widthAnchor),
            
            controlStack.heightAnchor.constraint(equalToConstant: 60)
        ])
    }
    
    // MARK: - Network Monitoring
    private func startNetworkMonitoring() {
        networkMonitor.pathUpdateHandler = { [weak self] path in
            DispatchQueue.main.async {
                self?.isNetworkAvailable = path.status == .satisfied
                self?.updateNetworkStatus()
            }
        }
        networkMonitor.start(queue: DispatchQueue.global(qos: .background))
    }
    
    private func updateNetworkStatus() {
        if !isNetworkAvailable {
            statusLabel.text = "Kh√¥ng c√≥ k·∫øt n·ªëi m·∫°ng - Ch·∫ø ƒë·ªô ngo·∫°i tuy·∫øn"
            statusLabel.textColor = .systemOrange
            statusLabel.isHidden = false
        } else {
            statusLabel.isHidden = true
        }
    }
    
    // MARK: - Speech Recognition Setup
    private func setupSpeechRecognition() {
        // Ki·ªÉm tra v√† y√™u c·∫ßu quy·ªÅn nh·∫≠n d·∫°ng gi·ªçng n√≥i
        SFSpeechRecognizer.requestAuthorization { [weak self] status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    self?.speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "ja-JP"))
                    // Ki·ªÉm tra xem nh·∫≠n d·∫°ng gi·ªçng n√≥i c√≥ kh·∫£ d·ª•ng kh√¥ng
                    if let speechRecognizer = self?.speechRecognizer, !speechRecognizer.isAvailable {
                        self?.offlineRecognitionActive = true
                        self?.statusLabel.text = "Nh·∫≠n d·∫°ng gi·ªçng n√≥i kh√¥ng kh·∫£ d·ª•ng - S·ª≠ d·ª•ng ch·∫ø ƒë·ªô ngo·∫°i tuy·∫øn"
                        self?.statusLabel.textColor = .systemOrange
                        self?.statusLabel.isHidden = false
                    }
                case .denied, .restricted:
                    let error = NSError(domain: "SpeechRecognition",
                                        code: 403,
                                        userInfo: [NSLocalizedDescriptionKey: "Quy·ªÅn nh·∫≠n d·∫°ng gi·ªçng n√≥i b·ªã t·ª´ ch·ªëi"])
                    self?.onError?(error)
                    self?.statusLabel.text = "Quy·ªÅn nh·∫≠n d·∫°ng gi·ªçng n√≥i b·ªã t·ª´ ch·ªëi"
                    self?.statusLabel.textColor = .systemRed
                    self?.statusLabel.isHidden = false
                case .notDetermined:
                    let error = NSError(domain: "SpeechRecognition",
                                        code: 401,
                                        userInfo: [NSLocalizedDescriptionKey: "Quy·ªÅn nh·∫≠n d·∫°ng gi·ªçng n√≥i ch∆∞a ƒë∆∞·ª£c x√°c ƒë·ªãnh"])
                    self?.onError?(error)
                @unknown default:
                    break
                }
            }
        }
    }
    
    // MARK: - Content Management
    private func loadContent() {
        guard let content = content else { return }
        
        DispatchQueue.main.async { [weak self] in
            guard let self = self else { return }
            self.imageView.image = UIImage(named: content.image)
            self.textLabel.text = content.questionText
            self.translationLabel.text = content.translation
        }
    }
    
    // MARK: - Button Actions
    @objc private func speakerTapped() {
        playAudio()
    }
    
    @objc private func microTapped() {
        if isRecording {
            stopRecording()
        } else {
            startRecording()
        }
    }
    
    @objc private func skipTapped() {
        onSkipRequest?()
    }
    
    // MARK: - Public Methods
    public func skipCurrent() {
        onSkipRequest?()
    }
    
    public func setSimilarityThreshold(_ threshold: Double) {
        similarityThreshold = max(0.0, min(1.0, threshold))
    }
    
    public func playCurrentAudio() {
        playAudio()
    }
    
    // MARK: - Audio Playback
    private func playAudio() {
        if let audioFileName = content?.audio {
            AudioUtils.shared.playSound(filename: audioFileName)
        }
    }
    
    // MARK: - Speech Recognition
    private func startRecording() {
        // Log ƒë·ªÉ debug
        print("‚è±Ô∏è B·∫Øt ƒë·∫ßu ghi √¢m...")
        
        // Reset previous session
        stopRecording()
        
        // Reset bi·∫øn l∆∞u k·∫øt qu·∫£ t·∫°m th·ªùi
        lastRecognizedText = ""
        
        // Kh·ªüi t·∫°o l·∫°i audio engine
        audioEngine = AVAudioEngine()
        guard let audioEngine = audioEngine,
              let speechRecognizer = speechRecognizer,
              speechRecognizer.isAvailable else {
            startOfflineRecording()
            return
        }
        
        do {
            // Thi·∫øt l·∫≠p audio session v·ªõi c·∫•u h√¨nh ƒë·∫ßy ƒë·ªß
            try AVAudioSession.sharedInstance().setCategory(.record, mode: .measurement, options: .duckOthers)
            try AVAudioSession.sharedInstance().setActive(true, options: .notifyOthersOnDeactivation)
            
            // Chu·∫©n b·ªã recognition request
            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            guard let recognitionRequest = recognitionRequest else {
                throw NSError(domain: "JapaneseReadingView",
                              code: 3,
                              userInfo: [NSLocalizedDescriptionKey: "Kh√¥ng th·ªÉ t·∫°o y√™u c·∫ßu nh·∫≠n d·∫°ng"])
            }
            
            // Thi·∫øt l·∫≠p t√πy ch·ªçn cho recognition request
            recognitionRequest.shouldReportPartialResults = true
            
            // Ki·ªÉm tra xem thi·∫øt b·ªã c√≥ h·ªó tr·ª£ nh·∫≠n d·∫°ng ngo·∫°i tuy·∫øn kh√¥ng
            if #available(iOS 13, *) {
                recognitionRequest.requiresOnDeviceRecognition = true
            }
            
            // L·∫•y input node
            let inputNode = audioEngine.inputNode
            
            // Chu·∫©n b·ªã recognition task v·ªõi h√†m x·ª≠ l√Ω k·∫øt qu·∫£ m·∫°nh m·∫Ω h∆°n
            recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
                guard let self = self else { return }
                
                if let result = result {
                    let recognizedText = result.bestTranscription.formattedString
                    
                    // C·∫≠p nh·∫≠t UI v·ªõi k·∫øt qu·∫£ t·∫°m th·ªùi
                    if !recognizedText.isEmpty {
                        self.lastRecognizedText = recognizedText
                        DispatchQueue.main.async {
                            self.statusLabel.text = "ƒêang nghe: \(recognizedText)"
                            self.statusLabel.textColor = .systemBlue
                            self.statusLabel.isHidden = false
                        }
                    }
                    
                    // N·∫øu l√† k·∫øt qu·∫£ cu·ªëi c√πng v√† c√≥ vƒÉn b·∫£n
                    if result.isFinal {
                        // S·ª≠ d·ª•ng k·∫øt qu·∫£ cu·ªëi ho·∫∑c k·∫øt qu·∫£ t·∫°m th·ªùi cu·ªëi c√πng
                        let finalText = recognizedText.isEmpty ? self.lastRecognizedText : recognizedText
                        if !finalText.isEmpty {
                            self.compareResult(finalText)
                        }
                        self.stopRecording()
                    }
                }
                
                // X·ª≠ l√Ω l·ªói
                if let error = error {
                    if let error = error as NSError?, error.domain == "kAFAssistantErrorDomain" {
                        if error.code == 203 {
                            // X·ª≠ l√Ω l·ªói "No speech detected"
                            DispatchQueue.main.async {
                                self.statusLabel.text = "Kh√¥ng ph√°t hi·ªán gi·ªçng n√≥i. Vui l√≤ng n√≥i to h∆°n."
                                self.statusLabel.textColor = .systemOrange
                                self.statusLabel.isHidden = false
                            }
                            
                            // N·∫øu c√≥ k·∫øt qu·∫£ t·∫°m th·ªùi, s·ª≠ d·ª•ng n√≥
                            if !self.lastRecognizedText.isEmpty {
                                self.compareResult(self.lastRecognizedText)
                            }
                        } else if error.code == 216 {
                            self.startOfflineRecording()
                        }
                    } else {
                        self.onError?(error)
                    }
                    
                    self.stopRecording()
                }
            }
            
            // C√†i ƒë·∫∑t tap tr√™n input node v·ªõi buffer size l·ªõn h∆°n
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            inputNode.installTap(onBus: 0, bufferSize: 4096, format: recordingFormat) { buffer, _ in
                self.recognitionRequest?.append(buffer)
            }
            
            // B·∫Øt ƒë·∫ßu audio engine
            audioEngine.prepare()
            try audioEngine.start()
            
            // C·∫≠p nh·∫≠t tr·∫°ng th√°i
            isRecording = true
            microButton.isSelected = true
            statusLabel.text = "ƒêang nghe..."
            statusLabel.textColor = .systemBlue
            statusLabel.isHidden = false
            
            // Th√™m timeout t·ª± ƒë·ªông ƒë·ªÉ tr√°nh treo
            DispatchQueue.main.asyncAfter(deadline: .now() + 10) { [weak self] in
                guard let self = self else { return }
                
                if self.isRecording {
                    print("‚è±Ô∏è Timeout t·ª± ƒë·ªông sau 10 gi√¢y")
                    
                    // N·∫øu c√≥ k·∫øt qu·∫£ t·∫°m th·ªùi, s·ª≠ d·ª•ng n√≥
                    if !self.lastRecognizedText.isEmpty {
                        print("üëâ S·ª≠ d·ª•ng k·∫øt qu·∫£ t·∫°m th·ªùi do timeout")
                        self.compareResult(self.lastRecognizedText)
                    }
                    
                    self.stopRecording()
                }
            }
            
        } catch {
            print("‚ùå L·ªói khi chu·∫©n b·ªã ghi √¢m: \(error.localizedDescription)")
            onError?(error)
            startOfflineRecording()
        }
    }
    
    
    // Ch·∫ø ƒë·ªô ghi √¢m ngo·∫°i tuy·∫øn (kh√¥ng s·ª≠ d·ª•ng nh·∫≠n d·∫°ng gi·ªçng n√≥i)
    private func startOfflineRecording() {
        audioEngine = AVAudioEngine()
        guard let audioEngine = audioEngine else { return }
        
        do {
            // Thi·∫øt l·∫≠p audio session
            try AVAudioSession.sharedInstance().setCategory(.record, mode: .measurement)
            try AVAudioSession.sharedInstance().setActive(true, options: .notifyOthersOnDeactivation)
            
            // L·∫•y input node
            let inputNode = audioEngine.inputNode
            
            // C√†i ƒë·∫∑t tap tr√™n input node (ch·ªâ ƒë·ªÉ ghi √¢m, kh√¥ng nh·∫≠n d·∫°ng)
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { _, _ in
                // Kh√¥ng l√†m g√¨ c·∫£, ch·ªâ ƒë·ªÉ ghi √¢m
            }
            
            // B·∫Øt ƒë·∫ßu audio engine
            audioEngine.prepare()
            try audioEngine.start()
            
            // C·∫≠p nh·∫≠t tr·∫°ng th√°i
            isRecording = true
            
            // Hi·ªÉn th·ªã th√¥ng b√°o
            statusLabel.text = "Ch·∫ø ƒë·ªô ngo·∫°i tuy·∫øn - Kh√¥ng c√≥ nh·∫≠n d·∫°ng gi·ªçng n√≥i"
            statusLabel.textColor = .systemOrange
            statusLabel.isHidden = false
            
            // T·ª± ƒë·ªông d·ª´ng sau 5 gi√¢y
            DispatchQueue.main.asyncAfter(deadline: .now() + 5) { [weak self] in
                guard let self = self, self.isRecording else { return }
                self.stopRecording()
                
                // Gi·∫£ l·∫≠p k·∫øt qu·∫£ (lu√¥n ƒë√∫ng trong ch·∫ø ƒë·ªô ngo·∫°i tuy·∫øn)
                self.onReadingResult?(true)
            }
            
        } catch {
            onError?(error)
            stopRecording()
        }
    }
    
    private func stopRecording() {
        print("‚èπÔ∏è D·ª´ng v√† d·ªçn d·∫πp ghi √¢m")
        
        // D·ª´ng v√† reset t·∫•t c·∫£ c√°c th√†nh ph·∫ßn ghi √¢m
        if let audioEngine = audioEngine, audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
        }
        
        // ƒê·∫£m b·∫£o k·∫øt th√∫c request v√† task
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionTask?.finish()
        
        // Reset c√°c bi·∫øn m·ªôt c√°ch r√µ r√†ng
        audioEngine = nil
        recognitionRequest = nil
        recognitionTask = nil
        
        // C·∫≠p nh·∫≠t tr·∫°ng th√°i
        isRecording = false
        microButton.isSelected = false
        
        // Reset audio session v·ªõi t√πy ch·ªçn notifyOthersOnDeactivation
        do {
            try AVAudioSession.sharedInstance().setActive(false, options: .notifyOthersOnDeactivation)
        } catch {
            print("‚ùå L·ªói khi reset audio session: \(error.localizedDescription)")
        }
        
        // ·∫®n th√¥ng b√°o ch·∫ø ƒë·ªô ngo·∫°i tuy·∫øn
        if !offlineRecognitionActive && isNetworkAvailable {
            statusLabel.isHidden = true
        }
    }
    
    
    // MARK: - Result Handling
    private func compareResult(_ recognizedText: String) {
        guard let content = content else { return }
        
        let similarity = calculateSimilarity(between: recognizedText, and: content.correctAnswer)
        onReadingResult?(similarity >= similarityThreshold)
    }
    
    private func calculateSimilarity(between str1: String, and str2: String) -> Double {
        // Implement text similarity algorithm - Levenshtein distance
        let distance = levenshteinDistance(str1.lowercased(), str2.lowercased())
        let maxLength = max(str1.count, str2.count)
        
        // Normalize to get similarity (0 to 1)
        return 1.0 - Double(distance) / Double(maxLength)
    }
    
    private func levenshteinDistance(_ s1: String, _ s2: String) -> Int {
        // X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát
        if s1.isEmpty { return s2.count }
        if s2.isEmpty { return s1.count }
        if s1 == s2 { return 0 }
        
        // S·ª≠ d·ª•ng chu·ªói ng·∫Øn h∆°n l√†m c·ªôt ƒë·ªÉ gi·∫£m kh√¥ng gian b·ªô nh·ªõ
        let (shorter, longer) = s1.count <= s2.count ? (Array(s1), Array(s2)) : (Array(s2), Array(s1))
        let m = shorter.count
        let n = longer.count
        
        // Ch·ªâ c·∫ßn l∆∞u tr·ªØ hai h√†ng c·ªßa ma tr·∫≠n thay v√¨ to√†n b·ªô ma tr·∫≠n
        var prevRow = Array(0...m)
        var currRow = Array(repeating: 0, count: m + 1)
        
        for i in 1...n {
            currRow[0] = i
            
            for j in 1...m {
                let cost = longer[i-1] == shorter[j-1] ? 0 : 1
                currRow[j] = min(
                    prevRow[j] + 1,        // X√≥a
                    currRow[j-1] + 1,      // Ch√®n
                    prevRow[j-1] + cost    // Thay th·∫ø ho·∫∑c gi·ªØ nguy√™n
                )
            }
            
            // Ho√°n ƒë·ªïi h√†ng cho l·∫ßn l·∫∑p ti·∫øp theo
            (prevRow, currRow) = (currRow, prevRow)
        }
        
        // K·∫øt qu·∫£ cu·ªëi c√πng n·∫±m trong prevRow v√¨ ƒë√£ ho√°n ƒë·ªïi ·ªü l·∫ßn l·∫∑p cu·ªëi
        return prevRow[m]
    }
}

// MARK: - AVAudioPlayerDelegate
extension JapaneseReadingView: AVAudioPlayerDelegate {
    public func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
        onPlaybackFinished?()
    }
    
    public func audioPlayerDecodeErrorDidOccur(_ player: AVAudioPlayer, error: Error?) {
        if let error = error {
            onError?(error)
        }
    }
}
